{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b14e397-6858-4bbb-89ba-fa9320746180",
   "metadata": {},
   "source": [
    "# Mini Challenge 1: Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13d9db-a26f-4f9d-a95e-d914fa678444",
   "metadata": {},
   "source": [
    "## Evaluating biases in a predictor for toxic text comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba1bcf-c1a6-45ac-95d9-d4aba6a5578c",
   "metadata": {},
   "source": [
    "In this mini-challenge, we are going to examine the biases when evaluating toxic text comments. \n",
    "\n",
    "In 2018 Jigsaw sponsored a challenge on <a href = \"https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/overview\"> Kaggle </a>  to classify toxic comments. In a <a href =\"https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data\"> 2019 follow-up challenge</a> data of forum comments was labeled on toxicity levels and identity groups with the goal of minimising unintended identity bias. <br>\n",
    "\n",
    "In this mini challenge we are going to delve into a part of the data set (shortened roughly 100 thousand of 2 million comments) and try to get an idea of how bias is represented in language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0ae72-90c7-4a26-be8c-d5cc46005af0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8971458-7b5d-4b5e-9213-9e8a85c51f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some libraries that are required or might prove helpful\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b94ee-e3fa-428a-a865-4a95b8c3c72a",
   "metadata": {},
   "source": [
    "## Reducing the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ef40d-20c4-4d6d-872e-49a57332f77c",
   "metadata": {},
   "source": [
    "The genuine data has been prepared as follows and provided to you in the file \"mini_data.csv\": \n",
    "\n",
    "```\n",
    "np.random.seed(0)\n",
    "\n",
    "full_data = pd.read_csv(\"./2017/all_data.csv\").dropna()\n",
    "data = pd.read_csv(\"./2017/all_data.csv\")[[\"comment_text\",\"toxicity\"]].dropna()\n",
    "\n",
    "toxics = (data[\"toxicity\"] > 0.7)\n",
    "racist_data = data[toxics]\n",
    "cleaner_data = data[~toxics]\n",
    "\n",
    "subset = shuffle(pd.concat([cleaner_data.sample(n=40000), racist_data]))\n",
    "\n",
    "subset.to_csv(\"./mini_data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278394cf-a957-4442-bec7-1ede76887713",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbaa6b-dcec-41f9-b093-6b1c95b6c682",
   "metadata": {},
   "source": [
    "We load the provided data set. We see that it has three columns \"original_id\" (int), \"comment_text\" (string) and \"toxicity\" (float in the interval [0,1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4da812-c964-4f35-b8f9-b84cd87d3d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./mini_data.csv\", header=0)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e650f-b8d5-4812-a7c0-b3d9b95a36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c199a4b-d2d2-4879-886e-e04163d999ab",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef819768-855b-4371-ac6c-49315dfc1c89",
   "metadata": {},
   "source": [
    "We choose the column \"comment_text\" as our independent values and \"toxicity\" as our dependent value. Comments with a toxicity score of 0.7 and above will be labelled as \"toxic\", the others as \"not toxic\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da5147-e503-4ae4-9462-62002058d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_cut = 0.7\n",
    "\n",
    "comments = dataset[\"comment_text\"]\n",
    "target = (dataset[\"toxicity\"] > toxicity_cut).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de2ba66-a28c-46fc-b53b-5f783206b258",
   "metadata": {},
   "source": [
    "If you want to check out some of the comments, run the following cells. Can you already get an idea how toxicity might be classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54c6bc-c05f-499d-b6c2-58f014de82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bools = (dataset[\"toxicity\"] > toxicity_cut)\n",
    "toxic_comments = dataset[bools]\n",
    "toxic_toxicity = dataset[\"toxicity\"][bools]\n",
    "\n",
    "non_toxic_comments = dataset[~bools]\n",
    "non_toxic_toxicity = dataset[\"toxicity\"][~bools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79446e2b-f60d-4d8d-984e-736e0b4efea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print comments labelled as toxic\n",
    "for i, (comment, tar) in enumerate(zip(toxic_comments[\"comment_text\"].iloc[0:10], toxic_toxicity.iloc[0:10])):\n",
    "        print(f\"{i} (score: {tar}): {comment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8924b7-6f72-44a9-9177-fcabda35498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print comments not labelled as toxic\n",
    "for i, (comment, tar) in enumerate(zip(non_toxic_comments[\"comment_text\"].iloc[0:10], non_toxic_toxicity.iloc[0:10])):\n",
    "        print(f\"{i} (score: {tar}): {comment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60759f0-5653-44b5-aa37-d347aa1590e9",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f5f9f-5f98-4b22-8702-fcfed604f5fc",
   "metadata": {},
   "source": [
    "For simplicity, we chose a logistic regression as this is a classification problem. This choice will show some drawbacks which are examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf08be-dad2-4c1e-b1ce-19b981adaa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed to get comparable results\n",
    "np.random.seed(0)\n",
    "\n",
    "# prepare the data\n",
    "comments_train, comments_test, y_train, y_test = train_test_split(comments, target, test_size = 0.3, stratify = target) \n",
    "\n",
    "# we use a count vectorizer to break the individual comments into tokens, one token per word\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(comments_train)\n",
    "\n",
    "# we create the training a matrix \n",
    "X_train = vectorizer.transform(comments_train)\n",
    "X_test = vectorizer.transform(comments_test)\n",
    "\n",
    "# train the classifier\n",
    "classifier = LogisticRegression(max_iter=2000)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805249d-6667-418d-8ee8-474b90718cd5",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3696ea-a9a8-4ccf-b740-a257fec8ad1f",
   "metadata": {},
   "source": [
    "The model classifies about 90% of the comments correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f677120-c6b1-445d-8a06-e6e70d4b4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the data\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50dab7-5df7-4b23-8a1c-c8e674b69b7f",
   "metadata": {},
   "source": [
    "A look at the confusion matrix shows that false positives and false negatives are about equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1308247-8115-4d5b-b8e8-9058e71c5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, classifier.predict(X_test))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366541e2-dc55-488d-891d-fca6c8cecb42",
   "metadata": {},
   "source": [
    "<h2>Data Wrangling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92695750-c284-4d62-9dc5-193b39b4f2ad",
   "metadata": {},
   "source": [
    "Let us have a look at some falsely identified comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9edfc6-0c72-4b62-897e-c7d95fc48e2d",
   "metadata": {},
   "source": [
    "**Question 1:** What might be potential reasons for false classifications by looking at the comments. Find three notable examples. Also classify the bias type and think of ways to prevent these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078fac6-5a7e-4f9f-9c9f-102748106bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine false negatives\n",
    "for k, (i, j) in enumerate(zip(y_test, X_test)):\n",
    "    if ( i and classifier.predict(j) == 0):\n",
    "        print(f\"{k}: {comments_test.iloc[k]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e8069-0273-4d85-bff7-1abcb7ce412e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine false positives\n",
    "for k, (i, j) in enumerate(zip(y_test, X_test)):\n",
    "    if (not i and classifier.predict(j) == 1):\n",
    "        print(f\"{k}: {comments_test.iloc[k]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cb64d-2251-4ccb-92b4-78688de049a2",
   "metadata": {},
   "source": [
    "## Examine the decision strategies of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96434c6f-6bd9-4689-af9f-8b85b9b36958",
   "metadata": {},
   "source": [
    "The model operates by stripping the input of its punctuation and case sensitivity, tokenises the words separated by spaces and assigns values to them that function as a \"toxicity score\" for the words. Let's take a look at the top 50 words the model considers as the best identifier for toxicity.\n",
    "\n",
    "**Question 2:** Can you find potential problems with some entries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02067a9-52d5-4a94-baf4-6ae2b92ab856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We are abusing some model internals by extracting the coefficents directly. \n",
    "# Each coefficient represent the weight an individual occurence of the word adds \n",
    "# to the decision whether the whole comment is toxic.\n",
    "\n",
    "coefficients = pd.DataFrame({\"word\": sorted(list(vectorizer.vocabulary_.keys())), \"coeff\": classifier.coef_[0]})\n",
    "\n",
    "coefficients.sort_values(by=['coeff']).tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76ab39-4535-4537-bb3c-a67c8cb03041",
   "metadata": {},
   "source": [
    "The following function predicts (based on our model) whether a string is considered toxic or not. We shall use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafe9a1-48c9-47b6-8c99-9d43519d57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify any string\n",
    "def classify_string(string, investigate=False):\n",
    "    prediction = classifier.predict(vectorizer.transform([string]))[0]\n",
    "    if prediction == 0:\n",
    "        print(\"NOT TOXIC:\", string)\n",
    "    else:\n",
    "        print(\"TOXIC:\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf473c-cf92-4520-9e66-de7208cb29fd",
   "metadata": {},
   "source": [
    "Test the model on some strings, start with \"i have a friend\". What happens if the friend is white, christian, muslim or black?</br>\n",
    "Does this imply that black people are talked about better than Muslims?\n",
    "What about statements like \"that is damn good\"?</br>\n",
    "\n",
    "\n",
    "Do not use interpunctation or capitalisation! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d0c22-d168-4596-9f07-5c320a6e431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of new_comment\n",
    "new_comment = \"i have a friend\"\n",
    "\n",
    "# Do not change the code below\n",
    "classify_string(new_comment)\n",
    "coefficients[coefficients.word.isin(new_comment.split())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564470cb-2e6d-4ee1-8e1d-b76c78643f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of new_comment\n",
    "new_comment = \"I have a christian friend\"\n",
    "\n",
    "# Do not change the code below\n",
    "classify_string(new_comment)\n",
    "coefficients[coefficients.word.isin(new_comment.split())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c30f1e-fa09-41c4-ae80-2b7a088b72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of new_comment\n",
    "new_comment = \"I have a black car\"\n",
    "\n",
    "# Do not change the code below\n",
    "classify_string(new_comment)\n",
    "coefficients[coefficients.word.isin(new_comment.split())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9d9cc-210b-4d6a-9b7d-ecbcfb53c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of new_comment\n",
    "new_comment = \"that is damn good\"\n",
    "\n",
    "# Do not change the code below\n",
    "classify_string(new_comment)\n",
    "coefficients[coefficients.word.isin(new_comment.split())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd256b0b-fdb7-46e4-8b29-e54d829cc237",
   "metadata": {},
   "source": [
    "Let's have a look at the coefficients for various words of interest. Feel free to add any words you deem interesting. </br>\n",
    "\n",
    "**Question3:** Discuss some scores. Put them into relation to words with similar meaning and think about the impact it has on the model's application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a726e-6a30-4f8f-b39f-413ce627ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of new_comment\n",
    "new_comment = \"black muslim faggot lesbian gay homosexual heterosexual indian native hispanic american mexican immigrant nazi military color colour cross halfmoon allah god trump hitler man woman male female\"\n",
    "coefficients[coefficients.word.isin(new_comment.split())].sort_values(by=['coeff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28863b91-8e1e-487a-a5b1-a6d98d822509",
   "metadata": {},
   "source": [
    "## Changing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404fe72-9a85-4836-a747-57276bfe78e9",
   "metadata": {},
   "source": [
    "**Question 4:** In the beginning, we used an arbitrary value of 0.7 as a threshold of determining toxic comments. What can you observe if you change the threshold to 0.6? Are the comments still clearly toxic? Have a look at the first 20 comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3932e1-aa8d-4bb3-878b-01c65444f6c8",
   "metadata": {},
   "source": [
    "## Identifying Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa6de1-69fc-4f38-bdf5-ede9de8a27d9",
   "metadata": {},
   "source": [
    "**Question 5:** Which biases can you find in the data set and the model? And why? Explicitly, think of:\n",
    "- Representation Bias\n",
    "- Selection Bias\n",
    "- Measurement Bias\n",
    "- Exclusion Bias\n",
    "- Algorithmic Bias\n",
    "- Historical Bias (opinions about topics might change over time)\n",
    "\n",
    "You can find the complete and original data set with its description <a href=\"https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data\"> here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d378f-a4cd-40c8-8cf9-f231df7fdddf",
   "metadata": {},
   "source": [
    "TEACHER ONLY\n",
    "\n",
    "Examples of reasons for bias:\n",
    "\n",
    "- Selection Bias: the data represents mostly British people with access to the internet that had an interest in the forum.\n",
    "- Representation Bias: those who don't fulfill the above criteria will be underrepresented\n",
    "- Measurement Bias: Spelling errors\n",
    "- Exclusion Bias: Potentially some extreme comments (death threats etc) were deleted by moderators\n",
    "- Algorithmic Bias: \"Go sleep with the fishes\" consists only of words the algorithm considers good. Can't differentiate between \"anti-xxx\" and \"xxx\".\n",
    "- Historical Bias: The data is a snapshot until 2017. Biases in Society that are prevalent at that time influence the data, especially when big events occur (\"migration crisis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a5378-79e6-40b9-b1f5-864b3c8179db",
   "metadata": {},
   "source": [
    "For the next task, be aware that the author of this exercise MIGHT have meddled with the data set in order to enhance some effects for didactic purposes.\n",
    "\n",
    "**Question 6:** How can you tell that the data was selected with that intent? Which biases are caused by the author maliciously manipulating the dataset for this exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64281d36-3aae-4c52-8267-9b4523e78a36",
   "metadata": {},
   "source": [
    "## Algorithmic bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bf198-9f68-473e-9ef2-4893a0f234ef",
   "metadata": {},
   "source": [
    "The model we chose, logistic regression, is quite a simple and linear model and shows clear limitations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820b5ce-be3c-4c32-9d94-f3a2c562e21b",
   "metadata": {},
   "source": [
    "**Question 7:** How does the classification works for long sentences or multiple occurences of the same word? How do combinations of words matter? \n",
    "\n",
    "Tipp: examine the following code with `toxicity_cut = 0.7` (it should switch from non-toxic to toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f43267-2998-445e-9aa5-0232054e58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_word =  \"cholestoral \"\n",
    "print(classify_string(magic_word * 61,))\n",
    "print(classify_string(magic_word * 62,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fcb57c-d02d-4e0b-ab23-9cc87e185e95",
   "metadata": {},
   "source": [
    "## Cultural Differences/Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a63cc-bbbd-48c9-8bfe-60302e15b877",
   "metadata": {},
   "source": [
    "**Questions 8:** Take this list of Australian swear words and check how the model judges them. Explain the process why it struggles to identify them and which bias is in play if you deploy it in an Australian context. How do you mitigate this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5997ff6-56db-4c76-b417-fef10be7584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "australian_swear_words = [\"bugger off\", \"fair suck of the sav\", \"get stuffed\", \"dads\", \"derro\", \"fanny\", \"drongo\", \"bogan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f248169-eb0e-4917-93db-b6c06034e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in australian_swear_words:\n",
    "    classify_string(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74d1b7-1cdc-4b93-b04b-a25db6d7fed5",
   "metadata": {},
   "source": [
    "**Question 9:** Think of a real world use case of an algorithm like this. How should it judge concerning false positives or negatives? Which control structures (human in/out of the loop etc) should accompany it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4701e0-af07-4dbe-a05c-59bdac152ad1",
   "metadata": {},
   "source": [
    "## Prevention/Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbca82-8a5b-463d-aa60-096b37153bb2",
   "metadata": {},
   "source": [
    "Consider <a href = \"https://ai.meta.com/blog/measure-fairness-and-mitigate-ai-bias/\">this </a> approach by Meta. \n",
    "\n",
    "**Question 10:** Discuss the effectivity and the implicit requirements. Consider the context of Meta's business model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc49e0a-297d-4218-bff7-e87facbb7f86",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d9780-8a3a-42f8-9db3-0606068a2cd9",
   "metadata": {},
   "source": [
    "**The original where this course is based on:**\n",
    "\n",
    "- [Identifying Bias in AI](https://www.kaggle.com/code/alexisbcook/identifying-bias-in-ai)\n",
    "\n",
    "**Some papers:**\n",
    "\n",
    "- [A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle](https://arxiv.org/pdf/1901.10002)\n",
    "- [Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification](https://storage.googleapis.com/gweb-research2023-media/pubtools/5007.pdf)\n",
    "- [Measuring and Mitigating Unintended Bias in Text Classification](https://storage.googleapis.com/gweb-research2023-media/pubtools/4578.pdf)\n",
    "\n",
    "**Tutorial about bias mitigation:**\n",
    "\n",
    "- [How To: Preprocessing for GloVe Part1: EDA](https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-for-glove-part1-eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730aefa-c7f9-42e9-9536-51c4026411f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gki-venv",
   "language": "python",
   "name": "gki-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
